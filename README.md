# ProKI Kafka InfluxDB

## Description

This project sets up a data pipeline that collects sensor data, sends it to Kafka, and stores it in a TimescaleDB database for visualization with Grafana. The pipeline is configured for multiple "chairs" and "machines", allowing for data collection from multiple sources.

## Architecture

The system architecture consists of the following key components:

*   **Kafka:** A distributed streaming platform for handling real-time data feeds.
*   **Kafka Connect:** A framework for connecting Kafka with external systems like databases.
*   **TimescaleDB:** A time-series database built on PostgreSQL for storing and analyzing time-series data.
*   **Grafana:** A data visualization tool for creating dashboards and monitoring data.
*   **Zookeeper:** A centralized service for maintaining configuration information, naming, providing distributed synchronization, and group services.
*   **Schema Registry:** Provides a serving layer for your Avro schemas. It stores a history of all schemas, provides multiple compatibility settings and allows evolution of schemas according to the configured compatibility.

## Data Flow

The data flow is as follows:

1.  Sensor data is generated by `producer.py` and sent to Kafka topics (e.g., `chair1.bigmachine`, `chair1.smallmachine`, `chair2.bigmachine`, `chair2.smallmachine`).
2.  The Kafka Connect JDBC Sink Connector consumes data from these topics and writes it to the corresponding tables in the TimescaleDB database.
3.  Grafana is configured to query the TimescaleDB database and visualize the data in dashboards.

## Configuration

### Kafka

Kafka is configured using the `docker-compose.yaml` file. Key configuration parameters include:

*   `KAFKA_ADVERTISED_LISTENERS`: Specifies the listeners that Kafka advertises to clients.
*   `KAFKA_ZOOKEEPER_CONNECT`: Specifies the Zookeeper connection string.

### TimescaleDB

TimescaleDB is configured using the `docker-compose.yaml` file and the SQL scripts in the `postgres/tables` directory. Key configuration parameters include:

*   `POSTGRES_DB`: Specifies the database name.
*   `POSTGRES_USER`: Specifies the database user.
*   `POSTGRES_PASSWORD`: Specifies the database password.

The SQL scripts create the necessary schemas, users, and tables in the database.

### Grafana

Grafana is configured using the `docker-compose.yaml` file and the files in the `grafana` directory. Key configuration parameters include:

*   `GF_AUTH_ANONYMOUS_ORG_ROLE`: Specifies the role for anonymous users.
*   `GF_AUTH_ANONYMOUS_ENABLED`: Enables anonymous access.
*   `GF_AUTH_BASIC_ENABLED`: Disables basic authentication.

The files in the `grafana` directory define the data sources and dashboards used for visualization.

### Kafka Connect

Kafka Connect is configured using the `kafka-init` directory. The `init.sh` scripts create the necessary Kafka topics and configure the JDBC Sink Connector. The `jdbc-connector.json` files define the connection parameters for the JDBC Sink Connector.

### Producer

The producer is configured using command line arguments.

*   `--chair`: Specifies the chair ID (e.g., chair1, chair2).
*   `--machine`: Specifies the machine type (e.g., bigmachine, smallmachine).
*   `--num_messages`: Specifies the number of messages to produce.

## Usage

1.  Start the services using `docker-compose up -d`.
2.  Run the producer using `python producer.py --chair <chair_id> --machine <machine_type> --num_messages <number_of_messages>`.
    *   Example: `python producer.py --chair chair1 --machine bigmachine --num_messages 1000`
3.  Access Grafana at `http://localhost:3000` and view the data in the dashboards.
4.  Access Kafka UI at `http://localhost:8080` to view the topics and schemas.

## Customization

To add a new chair or machine, you can follow these steps:

1.  **Kafka Connect Configuration:** Create a new directory in the `kafka-init/topics` directory for the new chair/machine combination (e.g., `kafka-init/topics/chair3/bigmachine`).
2.  **Configuration Files:** Create `init.sh`, `jdbc-connector.json`, and `jsonschema.json` files in the new directory, modifying them to reflect the new chair and machine names.
3.  **Producer Configuration:** Modify the `producer.py` script to produce data for the new chair and machine.
4.  **TimescaleDB Table Creation:** New TimescaleDB tables are also created by creating a new folder for the chair or machine and implementing the SQL schema or adding a new user.
5.  **Grafana Dashboards:** Dashboards can be handled the same way, by creating new dashboard files in the `grafana/dashboards` directory.

## License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.
